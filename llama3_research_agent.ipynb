{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c707a92-6f82-44d8-8de0-fa612064df5e",
   "metadata": {},
   "source": [
    "# Local Web Research Agent w/ Llama 3 8b\n",
    "\n",
    "### [Llama 3 Release](https://llama.meta.com/llama3/)\n",
    "\n",
    "### [Ollama Llama 3 Model](https://ollama.com/library/llama3)\n",
    "---\n",
    "\n",
    "![diagram](local_agent_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715059b3-857c-456d-a740-24e2551d739d",
   "metadata": {},
   "source": [
    "---\n",
    "[Llama 3 Prompt Format](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/)\n",
    "\n",
    "### Special Tokens used with Meta Llama 3\n",
    "* **<|begin_of_text|>**: This is equivalent to the BOS token\n",
    "* **<|eot_id|>**: This signifies the end of the message in a turn.\n",
    "* **<|start_header_id|>{role}<|end_header_id|>**: These tokens enclose the role for a particular message. The possible roles can be: system, user, assistant.\n",
    "* **<|end_of_text|>**: This is equivalent to the EOS token. On generating this token, Llama 3 will cease to generate more tokens.\n",
    "A prompt should contain a single system message, can contain multiple alternating user and assistant messages, and always ends with the last user message followed by the assistant header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "35f2cb84-6abf-4a6c-8d1f-cdc6474b77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying final output format\n",
    "from IPython.display import display, Markdown \n",
    "# LangChain Dependencies\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langgraph.graph import END, StateGraph\n",
    "# For State Graph \n",
    "from typing_extensions import TypedDict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d39b8539-1bfe-4001-b7b2-6752a77846d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"lsv2_pt_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "os.environ['LANGCHAIN_ENDPOINT']=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Research Agent\"\n",
    "os.environ['GROQ_API_KEY'] = 'gsk_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "9b341d1d-0a59-4c03-8558-759ea00171bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3-8b-8192\n"
     ]
    }
   ],
   "source": [
    "provider = 'groq'  # 'ollama' | 'lmstudio' | 'groq'\n",
    "\n",
    "# Defining LLM model names\n",
    "llm_model_name = \"llama3\" if provider == 'ollama' else 'QuantFactory/Meta-Llama-3-8B-Instruct-GGUF' if provider == 'lmstudio' else 'llama3-8b-8192'\n",
    "print(llm_model_name)\n",
    "\n",
    "# Initialize llm based on the provider\n",
    "if provider == 'ollama':\n",
    "    llm = ChatOllama(base_url=\"http://192.168.1.68:11434\",model=llm_model_name, temperature=0,)\n",
    "    llama3_json = ChatOllama(base_url=\"http://192.168.1.68:11434\",model=llm_model_name, format='json', temperature=0)\n",
    "elif provider == 'lmstudio':\n",
    "    base_url = \"http://192.168.1.68:1234/v1\"\n",
    "    api_key = \"LMSTUDIO\"\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_base=base_url,\n",
    "        openai_api_key=api_key,\n",
    "        model_name=llm_model_name\n",
    "    )\n",
    "    llama3_json = ChatOpenAI(\n",
    "        openai_api_base=base_url,\n",
    "        openai_api_key=api_key,\n",
    "        model_name=llm_model_name,\n",
    "        model_kwargs={\"response_format\": \"json\"},\n",
    "        temperature=0\n",
    "    )\n",
    "elif provider == 'groq':\n",
    "    llm = ChatGroq(\n",
    "        model_name=llm_model_name\n",
    "    )\n",
    "    llama3_json = ChatGroq(\n",
    "        model_name=llm_model_name,\n",
    "        temperature=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "4c7813ac-791f-4035-a5ec-04810d5de5f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Web Search Tool\n",
    "\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(max_results=25)\n",
    "web_search_tool = DuckDuckGoSearchRun(api_wrapper=wrapper)\n",
    "\n",
    "# Test Run\n",
    "# resp = web_search_tool.invoke(\"home depot news\")\n",
    "# resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "2d798a81-6ed6-4a4f-a1d9-93b4e3059fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation Prompt\n",
    "\n",
    "generate_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    <|begin_of_text|>\n",
    "    \n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    \n",
    "    You are an AI assistant for Research Question Tasks, that synthesizes web search results. \n",
    "    Strictly use the following pieces of web search context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    keep the answer concise, but provide all of the details you can in the form of a research report. \n",
    "    Only make direct references to material if provided in the context.\n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    \n",
    "    Question: {question} \n",
    "    Web Search Context: {context} \n",
    "    Answer: \n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "generate_chain = generate_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# Test Run\n",
    "# question = \"How are you?\"\n",
    "# context = \"\"\n",
    "# generation = generate_chain.invoke({\"context\": context, \"question\": question})\n",
    "# print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "49fa1965-6bd4-4dfc-9eb8-96c6cff7b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Router\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    <|begin_of_text|>\n",
    "    \n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    \n",
    "    You are an expert at routing a user question to either the generation stage or web search. \n",
    "    Use the web search for questions that require more context for a better answer, or recent events.\n",
    "    Otherwise, you can skip and go straight to the generation phase to respond.\n",
    "    You do not need to be stringent with the keywords in the question related to these topics.\n",
    "    Give a binary choice 'web_search' or 'generate' based on the question. \n",
    "    Return the JSON with a single key 'choice' with no premable or explanation. \n",
    "    \n",
    "    Question to route: {question} \n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "question_router = router_prompt | llama3_json | JsonOutputParser()\n",
    "\n",
    "# Test Run\n",
    "# question = \"What recently happened to donald trump?\"\n",
    "# print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "33ab4128-e0b0-4f49-9f36-1d3bf5636715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Transformation\n",
    "\n",
    "query_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    <|begin_of_text|>\n",
    "    \n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    \n",
    "    You are an expert at crafting web search queries for research questions.\n",
    "    More often than not, a user will ask a basic question that they wish to learn more about, however it might not be in the best format. \n",
    "    Reword their query to be the most effective web search string possible.\n",
    "    Return the JSON with a single key 'query' with no premable or explanation. \n",
    "    \n",
    "    Question to transform: {question} \n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "query_chain = query_prompt | llama3_json | JsonOutputParser()\n",
    "\n",
    "# Test Run\n",
    "# question = \"What's happened recently with trump?\"\n",
    "# print(query_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "a1c8e922-3f00-48d6-83cb-cc78a2292838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph State\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        search_query: revised question for web search\n",
    "        context: web_search result\n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    search_query : str\n",
    "    context : str\n",
    "\n",
    "# Node - Generate\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Step: Generating Final Response\")\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # Answer Generation\n",
    "    generation = generate_chain.invoke({\"context\": context, \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "# Node - Query Transformation\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform user question to web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended search query\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Step: Optimizing Query for Web Search\")\n",
    "    question = state['question']\n",
    "    gen_query = query_chain.invoke({\"question\": question})\n",
    "    search_query = gen_query[\"query\"]\n",
    "    return {\"search_query\": search_query}\n",
    "\n",
    "\n",
    "# Node - Web Search\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to context\n",
    "    \"\"\"\n",
    "\n",
    "    search_query = state['search_query']\n",
    "    print(f'Step: Searching the Web for: \"{search_query}\"')\n",
    "    \n",
    "    # Web search tool call\n",
    "    search_result = web_search_tool.invoke(search_query)\n",
    "    return {\"context\": search_result}\n",
    "\n",
    "\n",
    "# Conditional Edge, Routing\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    route question to web search or generation.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Step: Routing Query\")\n",
    "    question = state['question']\n",
    "    output = question_router.invoke({\"question\": question})\n",
    "    if output['choice'] == \"web_search\":\n",
    "        print(\"Step: Routing Query to Web Search\")\n",
    "        return \"websearch\"\n",
    "    elif output['choice'] == 'generate':\n",
    "        print(\"Step: Routing Query to Generation\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "8f665713-e80b-4d86-8015-77ba55506004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the nodes\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"websearch\", web_search)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# Build the edges\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"websearch\")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "local_agent = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "4f53aa05-20b2-420e-9a8f-bf12b1e547ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(query):\n",
    "    output = local_agent.invoke({\"question\": query})\n",
    "    print(\"=======\")\n",
    "    display(Markdown(output[\"generation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "6a1b135d-131e-4276-b40c-12ea8b78c39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: Routing Query\n",
      "Step: Routing Query to Web Search\n",
      "Step: Optimizing Query for Web Search\n",
      "Step: Searching the Web for: \"(\"\n",
      "Step: Generating Final Response\n",
      "=======\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided web search context, I couldn't find a specific research paper from 2024 onward that is widely considered the most disruptive in the field of AI/ML. However, I can suggest a few recent papers that have gained significant attention and have the potential to be considered as disruptive:\n",
       "\n",
       "1. \"Learning to Learn\" by Google Research (2022): This paper proposes a novel approach to learning in reinforcement learning environments. The authors introduce a new type of neural network called a \"memory-augmented neural network\" that can learn to learn and improve its performance over time.\n",
       "\n",
       "URL: https://arxiv.org/abs/2204.06606\n",
       "\n",
       "2. \"DALL-E: A Large-Scale Language Model for Image Generation\" by Meta AI (2022): This paper introduces DALL-E, a large-scale language model that can generate high-quality images from natural language descriptions. The model has the potential to revolutionize the field of computer vision and natural language processing.\n",
       "\n",
       "URL: https://arxiv.org/abs/2204.05375\n",
       "\n",
       "3. \"Improving Language Models by Unsupervised Pre-training with Multiple Tasks\" by Facebook AI (2022): This paper proposes a new approach to pre-training language models using multiple tasks and data sources. The authors show that this approach can improve the performance of language models on a variety of tasks, including language translation, text classification, and sentiment analysis.\n",
       "\n",
       "URL: https://arxiv.org/abs/2204.05857\n",
       "\n",
       "It's worth noting that the concept of \"disruptive\" in the context of AI/ML research is subjective and can vary depending on the perspective and criteria used to evaluate the impact of the research."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test it out!\n",
    "run_agent(\"Find the most disruptive AI/ML research paper from 2024 onward. Check the credibility of the researchers and provide a summary with the paper's URL.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
